---
title: "Independence of random variables"
description: "Definition of when random variables have factorizing joint probabilities."
---

A **family of random variables** \((X_i)_{i\in I}\) on a {{< knowl id="probability-space" text="probability space" >}} \((\Omega,\mathcal F,\mathbb P)\) is **independent** if for every finite choice of indices \(i_1,\dots,i_k\in I\) and every choice of Borel sets \(A_1,\dots,A_k\subseteq\mathbb R\),
\[
\mathbb P\big(X_{i_1}\in A_1,\dots,X_{i_k}\in A_k\big)=\prod_{j=1}^k \mathbb P\big(X_{i_j}\in A_j\big).
\]

This says that all events of the form \(\{X_i\in A\}\) behave like {{< knowl id="independence-events" text="independent events" >}} under {{< knowl id="event-probability" text="probability" >}}. Equivalently, the {{< knowl id="sigma-algebra" section="measure-theory" text="sigma-algebras" >}} \(\sigma(X_i)\) generated by the variables are {{< knowl id="independence-sigma-algebras" text="independent" >}}.

**Examples:**
- Let \(\Omega=\{0,1\}^2\) with \(\mathbb P\) uniform, and define \(X(\omega_1,\omega_2)=\omega_1\), \(Y(\omega_1,\omega_2)=\omega_2\). Then \(X\) and \(Y\) are independent {{< knowl id="random-variable" text="random variables" >}}.
- Let \(\Omega=[0,1]^2\) with the product {{< knowl id="lebesgue-measure" section="measure-theory" text="Lebesgue measure" >}} (normalized to a probability measure), and set \(X(u,v)=u\), \(Y(u,v)=v\). Then \(X\) and \(Y\) are independent and each has the uniform distribution on \([0,1]\).
